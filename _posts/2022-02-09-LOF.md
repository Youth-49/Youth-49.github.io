---
layout: post
title: Local Outlier Factor
date: 2022-02-09 12:00:00 +08:00
---

# 异常检测：LOF（Local Outlier Factor）



Local Outlier Factor（LOF）是基于密度的经典算法（Breuning et,al 2000）。之前的异常检测算法大都是基于统计，或借助聚类算法。但是基于统计的算法需要假设数据服从某种分布，聚类算法往往只能给出点是否是异常点，不能量化异常程度。而LOF则不对数据分布做太多要求，同时能给出每个点的异常程度。



## 算法步骤

### 相关定义

**K-distance**, $d_k(x)$: 距离点x第k近的点与点x的距离.

**K-distance neighborhood**, $N_k(x)$: 点x的k近邻，其中：


$$
|N_k(x)| \geq k
$$



**reach-distance**: 


$$
reach-dis_k(P, O) = \max\{d_k(O), d(P,O)\}
$$



k值越高，同一邻域内的点的可达距离将会越相似

**local reachability density**:


$$
lrd(P) = \frac{1}{\frac{\sum_{O \in N_k(P)}reach-dis_k(P,O)}{|N_k(P)|}}
$$



点P的局部可达密度是P与P的k近邻内点的平均reach-distance的倒数。

**local outlier factor**:

根据局部可达密度的定义，如果一个数据点跟其他点比较疏远的话，那么显然它的局部可达密度就小。**但LOF算法衡量一个数据点的异常程度，并不是看它的绝对局部密度，而是看它跟周围邻近的数据点的相对密度。**

这样做的好处是可以允许数据分布不均匀、密度不同的情况。局部异常因子即是用局部相对密度来定义的。



$$
LOF_k(P) = \frac{\frac{\sum_{O \in N_k(P)}lrd(O)}{|N_k(P)|}}{lrd(P)}
$$



### 算法

1. 对于每个数据点，计算它与其它所有点的距离，并按从近到远排序；
2. 对于每个数据点，找到它的 k-nearest-neighbor，计算 LOF 得分;
3. 如果LOF值越大，说明越异常，反之如果越小，说明越趋于正常。



## 优缺点

### 优点

LOF 的一个优点是它同时考虑了数据集的局部和全局属性。异常值不是按绝对值确定的，而是相对于它们的邻域点密度确定的。当数据集中存在不同密度的不同集群时，LOF表现仍然良好，比较适用于中等高维的数据集。



### 缺点

LOF算法中关于局部可达密度的定义其实暗含了一个假设，即：**不存在大于等于 k 个重复的点。**

当这样的重复点存在的时候，这些点的平均可达距离为零，局部可达密度就变为无穷大，会给计算带来一些麻烦。在实际应用时，为了避免这样的情况出现，可以把 k-distance 改为 k-distinct-distance，不考虑重复的情况。或者，还可以考虑给可达距离都加一个很小的值，避免可达距离等于零。

另外，LOF 算法需要计算数据点两两之间的距离，造成整个算法时间复杂度为$O(N^2)$ 。为了提高算法效率，后续有算法尝试改进。FastLOF （Goldstein，2012)先将整个数据随机的分成多个子集，然后在每个子集里计算 LOF 值。对于那些 LOF 异常得分小于等于 1 的，从数据集里剔除，剩下的在下一轮寻找更合适的 nearest-neighbor，并更新 LOF 值。



## 应用

可用于异常检测，数据清洗。

异常检测（anomaly detection）又分为异常值检测和新颖值（奇异值）检测：

1. 异常值检测（Outlier detection）：训练数据中含有异常值，通过相关算法找到训练数据的中心模式，忽略偏差观测值，从而检测出异常值。
2. 奇异值检测（Novelty detection）：训练数据不包含异常值，只含有positive（正常）的数据，通过算法学习其pattern。之后用于检测未曾看到过新数据是否属于这个pattern，如果属于，该新数据是positive，否则negative，即奇异值。


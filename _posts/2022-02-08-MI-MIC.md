---
layout: post
title: Mutual Information and Maximum Information Coeffient
date: 2022-02-09 17:00:00 +08:00

---



# 特征选择-互信息与最大信息系数



## Pre-requisite

香农首先定义了事件的信息量为$\log{\frac1p}$

信息论中定义了**熵**用来描述事件的不确定性，也是所发生的事件的信息量的期望：


$$
\begin{align}
H(X) &= \sum_{k=1}^NP(X = k)\log_2{\frac{1}{P(X=k)}} \\ &= -\sum_{k=1}^NP(X = k)\log_2{P(X=k)} \\
&= -E[\log P(X)] \\
\end{align}
$$


均匀分布的熵最大。



**条件熵**：


$$
\begin{align}
H(Y|X) &= \sum_{x \in X}P(x)H(Y|X=x)\\
&= \sum_{x \in X}P(x)\sum_{y \in Y}p(y|x)\log\frac1{P(y|x)} \\
&= \sum_{x\in X}\sum_{y \in Y}P(x)P(y|x)\log{\frac1{P(y|x)}} \\
&= \sum_{x\in X}\sum_{y \in Y}P(x,y)\log{\frac1{P(y|x)}} \\
&= -E[\log P(Y|X)]
\end{align}
$$


**联合熵**：


$$
H(X, Y) = \sum_{x\in X}\sum_{y \in Y}P(x,y)\log{\frac1{P(x, y)}} = -E[\log P(X,Y)]
$$


**交叉熵**：


$$
H_{CE}(X,Y) = -\sum_{k=1}^N P(X=k)\log_2{P(Y=k)}
$$


用于衡量两个随机变量的概率分布的一致性。如果两个变量同分布，则$H_{CE}(X, Y) = H(X) = H(Y)$。

两个随机变量概率分布的差异可由**KL散度**量化，KL散度又称为**相对熵**：


$$
KL(X,Y) =D_{KL}(p||q) = \sum_ip(x_i)(\log_2\frac{1}{q(x_i)}-\log_2\frac1{p(x_i)}) = H_{CE}(X,Y)-H(X)
$$


当两个分布相同时，KL散度最小，为0。注意相对熵（KL散度）是一个非对称的度量。

这是一篇讲解KL散度的博客：[如何理解K-L散度（相对熵） - 简书 (jianshu.com)](https://www.jianshu.com/p/43318a3dc715)

为解决相对熵不对称的问题，提出了**JS散度**：


$$
D_{JS}(p(x), q(x)) = 0.5*[D_{KL}(p||\frac{p+q}{2}) + D_{KL}(q||\frac{p+q}{2})]
$$


**信息增益（Information Gain）**：用来描述一个变量对另一个变量的影响，变量A对D的信息增益为：


$$
g(D, A) = H(D) - H(D|A)
$$


经典的**互信息**也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：


$$
I(X,Y) = \sum_{x\in X, y \in Y}p(x,y)\log_2{\frac{p(x,y)}{p(x)p(y)}} \\
I(X,Y) = I(Y, X)
$$


互信息可以理解为X和Y联合分布和X、Y边缘分布之积的KL散度。

X为待选择的特征，Y为标签或预测值。二者互信息越大，说明相关性越强。

对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。

**最大信息系数（MIC）**克服了这个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。

MIC首先将连续的取值离散化，采用的是分箱法。

对于连续的随机变量，互信息为：


$$
I(X, Y) = \int p(x, y)\log_2{\frac{p(x,y)}{p(x)p(y)}}dxdy
$$


用类似蒙特卡洛的思想，将X-Y画出散点图，在图上划分网格，用离散的散点近似联合分布。当数据量足够大时，这种近似的效果越好。

最后要进行归一化，以消除近似中网格密度得影响，即最大化：


$$
MIC(X, Y) = \max_{a*b \leq B} \frac{I(X, Y)}{log2\min{(a, b)}}
$$


经验上地，B为数据量的0.6次方。

[MIC：最大信息系数_满腹的小不甘-CSDN博客_最大信息系数](https://blog.csdn.net/qq_27586341/article/details/90603140)



### Question

为什么要取max？分格近似一定会比真实值小吗